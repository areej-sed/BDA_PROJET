# Streaming Taxi avec Kafka, MongoDB et Cassandra

Projet du module **BDABD**

---

## 1. Description du projet

Ce projet a pour objectif de ....

---

## 1.1 Structure du projet

Voici l'arborescence des fichiers importants et leur description :

```
BDA_PROJET/
â”œâ”€â”€ data/                       # Dossier de stockage des donnÃ©es
â”‚   â”œâ”€â”€ input/                  # DonnÃ©es brutes (Parquet, Shapefiles)
â”‚   â””â”€â”€ output/                 # DonnÃ©es traitÃ©es (JSON)
â”‚
â”œâ”€â”€ data_preparation/           # Scripts de prÃ©paration des donnÃ©es
â”‚   â””â”€â”€ data_preprocessing.ipynb # Notebook principal pour le nettoyage et la conversion des donnÃ©es
â”‚
â”œâ”€â”€ docker/                     # Configuration des conteneurs Docker
â”‚   â”œâ”€â”€ kafka/                  # Environnement Kafka
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml  # DÃ©ploiement du cluster Kafka (Zookeeper, Brokers, Control Center)
â”‚   â”‚   â””â”€â”€ producer/           # Producer Kafka (Python)
â”‚   â”‚       â”œâ”€â”€ producer.py     # Script d'envoi des donnÃ©es vers Kafka
â”‚   â”‚       â”œâ”€â”€ Dockerfile      # Image Docker du producer
â”‚   â”‚       â””â”€â”€ requirements.txt # DÃ©pendances Python du producer
â”‚   â”‚
â”‚   â””â”€â”€ mongodb/                # Environnement MongoDB
â”‚       â”œâ”€â”€ consumer/           # Consumer Kafka vers MongoDB (Python)
â”‚       â”‚   â”œâ”€â”€ consumer.py     # Script d'ingestion des donnÃ©es
â”‚       â”‚   â”œâ”€â”€ Dockerfile      # Image Docker du consumer
â”‚       â”‚   â””â”€â”€ requirements.txt # DÃ©pendances Python du consumer
â”‚       â”œâ”€â”€ docker-compose.yml  # DÃ©ploiement du cluster MongoDB (Config, Shards, Router, Consumer)
â”‚       â””â”€â”€ init-mongo.sh       # Script d'initialisation
â”‚
â””â”€â”€ README.md                   # Documentation du projet
```

---

## 2. PrÃ©requis

Avant dâ€™exÃ©cuter lâ€™application, il est nÃ©cessaire dâ€™avoir installÃ© :

- **Python 3.8 ou supÃ©rieur**
- **pip**

- **Docker Desktop**

### BibliothÃ¨ques Python requises

- pandas
- pyarrow
- numpy
- python-dateutil
- tqdm
- ...

---

## 3. Installation et exÃ©cution

### Ã‰tape 1 : Cloner ou tÃ©lÃ©charger le projet

```bash
git clone https://github.com/areej-sed/taxi_streaming_project
cd taxi_streaming_project
```

**Ã‰tape 2 : CrÃ©er et activer un environnement virtuel (optionnel mais recommandÃ©)**

```bash
python -m venv venv
```

\*Activation

```bash
venv\Scripts\activate
```

**Ã‰tape 3 : Installer les dÃ©pendances**

```bash
pip install pandas pyarrow numpy python-dateutil tqdm
```

## 4.

## Partie 1 : PrÃ©paration des donnÃ©es

Toutes les Ã©tapes de prÃ©traitement des donnÃ©es se trouvent dans le notebook Jupyter :
**`data_preparation/data_preprocessing.ipynb`**

Les principales Ã©tapes rÃ©alisÃ©es dans ce notebook sont :
- Chargement des donnÃ©es brutes (Parquet) et des zones de taxi (Shapefile).
- Calcul des latitudes et longitudes (centroÃ¯des) Ã  partir des gÃ©omÃ©tries des zones.
- Enrichissement des donnÃ©es : jointure pour ajouter les coordonnÃ©es et les noms de quartiers (Boroughs/Zones) aux points de prise en charge et de dÃ©pose.
- SÃ©lection des colonnes pertinentes et renommage pour uniformisation.
- Ajout d'un identifiant unique `trip_id`.
- Export des donnÃ©es traitÃ©es en fichier JSON (`final_data.json`) pour l'ingestion.


## Partie 2 : Mise en place du Cluster Kafka avec Docker

**Ã‰tape 1 : Inspecter le fichier Parquet**

```bash
cd docker/kafka
docker compose up -d
```

**verefication**

```bash
docker ps
```

**verefication des logs**

```bash
docker logs kafka1
```

**Ã‰tape 2 : CrÃ©ation du topic Kafka taxi.raw**

CrÃ©ation du topic :
```bash
docker exec -it kafka1 kafka-topics --create --topic taxi.raw --bootstrap-server kafka1:9092 --partitions 6 --replication-factor 2
```

VÃ©rifier si le topic existe :
```bash
docker exec -it kafka1 kafka-topics --list --bootstrap-server kafka1:9092
```

Supprimer le topic (si nÃ©cessaire pour rÃ©initialiser les donnÃ©es) :
```bash
docker exec -it kafka1 kafka-topics --delete --topic taxi.raw --bootstrap-server kafka1:9092
```

**Ã‰tape 3 : Lancer le Producer**

```bash
docker compose up producer
```

---

## Partie 3 : Cluster MongoDB (Sharding) et Consumer

**Ã‰tape 1 : DÃ©marrage du cluster MongoDB**

```bash
cd docker/mongodb
docker-compose up -d
```

**Ã‰tape 2 : Initialisation du Config Server**

```bash
docker exec -it mongo-config mongosh --port 27019 --eval "rs.initiate({_id:'configrs',configsvr:true,members:[{_id:0,host:'mongo-config:27019'}]})"
```

_Attendre 5 secondes..._

```powershell
Start-Sleep -Seconds 5
# ou "sleep 5" sur Linux/Mac
```

**Ã‰tape 3 : Initialisation des Shards**

```bash
docker exec -it mongo-shard1 mongosh --port 27018 --eval "rs.initiate({_id:'shard1rs',members:[{_id:0,host:'mongo-shard1:27018'}]})"
docker exec -it mongo-shard2 mongosh --port 27018 --eval "rs.initiate({_id:'shard2rs',members:[{_id:0,host:'mongo-shard2:27018'}]})"
docker exec -it mongo-shard3 mongosh --port 27018 --eval "rs.initiate({_id:'shard3rs',members:[{_id:0,host:'mongo-shard3:27018'}]})"
```

_Attendre 15 secondes..._

```powershell
Start-Sleep -Seconds 15
# ou "sleep 15" sur Linux/Mac
```

**Ã‰tape 4 : Ajout des Shards au Router**

```bash
docker exec -it mongos mongosh --eval "sh.addShard('shard1rs/mongo-shard1:27018')"
docker exec -it mongos mongosh --eval "sh.addShard('shard2rs/mongo-shard2:27018')"
docker exec -it mongos mongosh --eval "sh.addShard('shard3rs/mongo-shard3:27018')"
```

**Ã‰tape 5 : Activation du Sharding**

```bash
docker exec -it mongos mongosh --eval "sh.enableSharding('taxi_streaming')"
```

**Ã‰tape 6 : CrÃ©ation de la collection et de la clÃ© de sharding**

```bash
docker exec -it mongos mongosh --eval "use taxi_streaming; db.createCollection('taxi_events'); db.taxi_events.createIndex({trip_id:1},{unique:true}); db.taxi_events.createIndex({pickup_borough_id:1}); sh.shardCollection('taxi_streaming.taxi_events',{pickup_borough_id:1})"
```

**VÃ©rification du statut du Sharding**

```bash
docker exec -it mongos mongosh --eval "sh.status()"
```

**Ã‰tape 7 : DÃ©marrage du Consumer**

```bash
docker restart mongo-consumer
docker logs -f mongo-consumer
```

_Sortie attendue :_
```
âœ… Connected to MongoDB
âœ… Connected to Kafka
ðŸ”„ Listening for messages...
âœ… Inserted 100 events | Total: 100
...
```

**VÃ©rification finale : Comptage des documents**

```bash
docker exec -it mongos mongosh --eval "use taxi_streaming; db.taxi_events.countDocuments()"
```
